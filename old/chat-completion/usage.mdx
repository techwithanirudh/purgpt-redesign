import { Tabs, Tab } from "../../../components/things-from-nextra-that-arent-importing-nicely/tabs.jsx";

# Using Chat Completion

The URL to send the POST request to is `https://purgpt.xyz/v1/chat/completions`.

To use PurGPT's completion models, there are certain required fields to include in the body of the request:

```json
{
    "messages": "AN ARRAY OF PREVIOUS MESSAGES", // ex: [{ "role": "user", "content": "Hello, PurGPT!" }]
    "model": "ANY DESIRED MODEL" // ex: gpt-3.5-turbo
} 
```

Given that, here is a table with more information on both required and optional parameters that can be passed into the body:

## API Reference

| **Field**                   | **Type**                  | **Description**                                                                                                                                         |
|-------------------------|-----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|
| **model**               | `string`              | **Required**. ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.                |
| **messages**            | `array`               | **Required**. A list of messages comprising the conversation so far. See [`messages` API Reference](#messages-api-reference)                                                     |
| **functions**           | `array`               | Optional. A list of functions the model may generate JSON inputs for. See [`functions` API Reference](#functions-api-reference)                                                                               |
| **function_call**       | `string` or `object`  | Optional. Controls how the model responds to function calls. "none" means the model does not call a function, and responds to the end-user. "auto" means the model can pick between an end-user or calling a function. Specifying a particular function via `{"name": "my_function"}` forces the model to call that function. "none" is the default when no functions are present. "auto" is the default if functions are present. |
| **temperature**         | `number`              | Optional. Defaults to 1. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. |
| **top_p**               | `number`              | Optional. Defaults to 1. An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both. |
| **n**                   | `integer`             | Optional. Defaults to 1. How many chat completion choices to generate for each input message.                                                        |
| **stream**              | `boolean`             | Optional. Defaults to false. If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. |
| **stop**                | `string` or `array`   | Optional. Defaults to null. Up to 4 sequences where the API will stop generating further tokens.                                                     |
| **max_tokens**          | `integer`             | Optional. Defaults to inf. The maximum number of [tokens](https://platform.openai.com/tokenizer) to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length. |
| **presence_penalty**    | `number`              | Optional. Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. [See more information about frequency and presence penalties.](https://platform.openai.com/docs/api-reference/parameter-details) |
| **frequency_penalty**   | `number`              | Optional. Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. [See more information about frequency and presence penalties.](https://platform.openai.com/docs/api-reference/parameter-details) |
| **logit_bias**          | `map`                 | Optional. Defaults to null. Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. |
| **user**                | `string`              | Optional. A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).                                |

### `messages` API Reference

| **Field**                   | **Type**                  | **Description**                                                                                                                                         |
|-------------------------|-----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|
| **role**                | `string`              | **Required**. The role of the messages author. One of `system`, `user`, `assistant`, or `function`.                                                        |
| **content**             | `string`              | **Required**. The contents of the message. `content` is required for all messages, and may be null for assistant messages with function calls.        |
| **name**                | `string`              | Optional. The name of the author of this message. `name` is required if role is `function`, and it should be the name of the function whose response is in the `content`. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters. |
| **function_call**       | `object`              | Optional. The name and arguments of a function that should be called, as generated by the model.                                                     |

### `functions` API Reference

| **Field**                   | **Type**                  | **Description**                                                                                                                                         |
|-------------------------|-----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|
| **name**                | `string`              | **Required**. The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.           |
| **description**         | `string`              | Optional. A description of what the function does, used by the model to choose when and how to call the function.                                     |
| **parameters**          | `object`              | **Required**. The parameters the functions accepts, described as a JSON Schema object. See the [guide](https://platform.openai.com/docs/guides/gpt/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.                                                                                                  |


## Code Example

<Tabs items={["JavaScript/Ajax", "Node.js", "Python", "C#", "cURL/BASH"]}>
  <Tab>
   ```js copy filename="index.js"
    const url = 'https://purgpt.xyz/v1/chat/completions';

    const data = JSON.stringify({
        "messages": [{ "role": "user", "content": "Hello, PurGPT!" }],
        "model": "gpt-3.5-turbo" // ANY DESIRED MODEL
    });

    const response = await fetch(url, {
      method: 'POST',
      headers: {
          'Content-Type': 'application/json',
          'Authorization': 'Bearer YOUR API KEY',
      },
        body: data,
    });

    const text = await response.text();

    console.log(text);
    ```
  </Tab>
  <Tab>
   ```js copy filename="index.js"
    const http = require("https");

    const url = 'https://purgpt.xyz/v1/chat/completions';

    const options = {
        method: 'POST',
        'Content-Type': 'application/json',
        'Authorization': 'Bearer YOUR API KEY',
    };

    const data = JSON.stringify({
        "messages": [{ "role": "user", "content": "Hello, PurGPT!" }],
        "model": "gpt-3.5-turbo" // ANY DESIRED MODEL
    });

    let result = '';
    const req = http.request(url, options, (res) => {
        console.log(res.statusCode);

        res.setEncoding('utf8');
        res.on('data', (chunk) => {
            result += chunk;
        });

        res.on('end', () => {
            console.log(result);
        });
    });

    req.on('error', (e) => {
        console.error(e);
    });

    req.write(data);
    req.end();
    ```
    </Tab>
    <Tab>
   ```py copy filename="index.py"
    import requests
    from requests.structures import CaseInsensitiveDict

    url = "https://purgpt.xyz/v1/chat/completions"

    headers = CaseInsensitiveDict()
    headers["Content-Type"] = "application/json"
    headers["Authorization"] = "Bearer YOUR API KEY"

    data = """
    {
            "messages": [{ "role": "user", "content": "Hello, PurGPT!" }],
            "model": "gpt-3.5-turbo" // ANY DESIRED MODEL
        }
    """

    resp = requests.post(url, headers=headers, data=data)

    print(resp.status_code)
    ```
    </Tab>
    <Tab>
     ```cs copy filename="index.cs" 
    var url = "https://purgpt.xyz/v1/chat/completions";

    var httpRequest = (HttpWebRequest)WebRequest.Create(url);
   
    httpRequest.Method = "POST";
    httpRequest.Headers["Authorization"] = "Bearer YOUR API KEY";
    httpRequest.ContentType = "application/json";

    var data = @"{
            ""messages"": [{ ""role"": ""user"", ""content"": ""Hello, PurGPT!"" }],
            ""model"": ""gpt-3.5-turbo"" // ANY DESIRED MODEL
        }";

    using (var streamWriter = new StreamWriter(httpRequest.GetRequestStream()))
    {
         streamWriter.Write(data);
    }

    var httpResponse = (HttpWebResponse)httpRequest.GetResponse();
    using (var streamReader = new StreamReader(httpResponse.GetResponseStream()))
    {
         var result = streamReader.ReadToEnd();
    }

    Console.WriteLine(httpResponse.StatusCode);
    ```
    </Tab>
    <Tab>
    ```bash copy filename="Console"
    curl -X POST https://purgpt.xyz/v1/chat/completions -H "Content-Type: application/json" -H "Authorization: Bearer YOUR API KEY" --data-binary @- <<DATA
        {
           "messages": [{ "role": "user", "content": "Hello, PurGPT!" }],
           "model": "gpt-3.5-turbo" // ANY DESIRED MODEL
        }
    DATA
    ```
    </Tab>
</Tabs>